{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/death-reaper/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_hand_written_data/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_hand_written_data/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_hand_written_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_hand_written_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"mnist_hand_written_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fcbf3696400>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fcbf3696438>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fcbf37120f0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(X_img, is_var_scope_exist=False):\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:\n",
    "    \n",
    "        ## First convolutional neural network\n",
    "        disc_weight0 = tf.get_variable(\"disc_weight0\", \n",
    "                                      [5, 5, 1, 32], \n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        disc_bias0 = tf.get_variable(\"disc_bias0\", \n",
    "                                    [32], \n",
    "                                    initializer = tf.constant_initializer(value=0))\n",
    "\n",
    "        disc_conv_weight0 = tf.nn.conv2d(input=X_img, \n",
    "                                         filter=disc_weight0, \n",
    "                                         strides = [1, 1, 1, 1], \n",
    "                                         padding=\"SAME\")\n",
    "\n",
    "        disc_conv_weight0 = disc_conv_weight0 + disc_bias0\n",
    "        disc_activated0 = tf.nn.relu(disc_conv_weight0)\n",
    "\n",
    "        disc_conv_net0 = tf.nn.max_pool(disc_activated0, \n",
    "                                     ksize=[1, 2, 2, 1], \n",
    "                                     strides = [1, 2, 2, 1], \n",
    "                                     padding=\"SAME\")\n",
    "\n",
    "        ## Second convolutional neural network\n",
    "        disc_weight1 = tf.get_variable(\"disc_weight1\", \n",
    "                                       [5, 5, 32, 64], \n",
    "                                       initializer = tf.truncated_normal_initializer(stddev = 0.01))\n",
    "\n",
    "        disc_bias1 = tf.get_variable(\"disc_bias1\", \n",
    "                                     [64], \n",
    "                                     initializer=tf.constant_initializer(value=0))\n",
    "\n",
    "        disc_conv_weight1 = tf.nn.conv2d(input=disc_conv_net0, \n",
    "                                         filter = disc_weight1, \n",
    "                                         strides = [1, 1, 1, 1], \n",
    "                                         padding = \"SAME\")\n",
    "        disc_conv_weight1 = disc_conv_weight1 + disc_bias1\n",
    "        disc_activated1 = tf.nn.relu(disc_conv_weight1)\n",
    "        disc_conv_net1 = tf.nn.max_pool(disc_activated1, \n",
    "                                         ksize=[1,2,2,1], \n",
    "                                         strides=[1,2,2,1], \n",
    "                                         padding=\"SAME\")\n",
    "\n",
    "        ## First fully connected layer\n",
    "        disc_weight3 = tf.get_variable(\"disc_weight3\", \n",
    "                                       [7*7*128, 1024], \n",
    "                                       initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        disc_bias3 = tf.get_variable(\"disc_bias3\", \n",
    "                                     [1024], \n",
    "                                     initializer=tf.constant_initializer(value = 0))\n",
    "        disc_conv_net1_reshaped = tf.reshape(disc_conv_net1, [-1, 7*7*128])\n",
    "\n",
    "        disc_fully_con1 = tf.matmul(disc_conv_net1_reshaped, disc_weight3)\n",
    "        disc_fully_con1 = disc_fully_con1 + disc_bias3\n",
    "        disc_fully_con1 = tf.nn.relu(disc_fully_con1)\n",
    "\n",
    "        ## Second fully connected layer\n",
    "        disc_weight4 = tf.get_variable(\"disc_weight4\", \n",
    "                                       [1024, 128], \n",
    "                                       initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        disc_bias4 = tf.get_variable(\"disc_bias4\", \n",
    "                                     [128], \n",
    "                                     initializer=tf.constant_initializer(value = 0))\n",
    "\n",
    "        disc_fully_con2 = tf.matmul(disc_fully_con1, disc_weight4)\n",
    "        disc_fully_con2 = disc_fully_con2 + disc_bias4\n",
    "\n",
    "\n",
    "        ## Third fully connected layer\n",
    "        disc_weight5 = tf.get_variable(\"disc_weight5\", \n",
    "                                       [128, 1], \n",
    "                                       initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        disc_bias5 = tf.get_variable(\"disc_bias5\", \n",
    "                                     [1], \n",
    "                                     initializer=tf.constant_initializer(value = 0))\n",
    "\n",
    "        disc_fully_con3 = tf.matmul(disc_fully_con2, disc_weight5)\n",
    "        disc_fully_con3 = disc_fully_con3 + disc_bias5\n",
    "\n",
    "        return disc_fully_con3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(batch_size, noise_dim):\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:\n",
    "        \n",
    "        noise = tf.truncated_normal([batch_size, noise_dim], name = \"noise\")\n",
    "\n",
    "        ## deconvolutional neural network 0\n",
    "        gen_weight0 = tf.get_variable(\"gen_weight0\", \n",
    "                                      [noise_dim, 3136], \n",
    "                                      initializer=tf.truncated_normal_initializer(stddev = 0.01), \n",
    "                                      dtype = tf.float32)\n",
    "\n",
    "        gen_bias0 = tf.get_variable(\"gen_bias0\", \n",
    "                                    [3136], \n",
    "                                    initializer = tf.truncated_normal_initializer(stddev = 0.01))\n",
    "        gen_network0 = tf.matmul(noise, gen_weight0) + gen_bias0\n",
    "        gen_network0 = tf.reshape(gen_network0, [-1, 56, 56, 1])\n",
    "        gen_network0 = tf.contrib.layers.batch_norm(gen_network0, epsilon=1e-5, scope = \"bn0\")\n",
    "        gen_network0 = tf.nn.relu(gen_network0)\n",
    "\n",
    "        ## deconvolutional neural network 1\n",
    "        gen_weight1 = tf.get_variable(\"gen_weight1\", \n",
    "                                      [3, 3, 1, noise_dim/2], \n",
    "                                      initializer=tf.truncated_normal_initializer(stddev = 0.01), \n",
    "                                      dtype = tf.float32)\n",
    "\n",
    "        gen_bias1 = tf.get_variable(\"gen_bias1\", \n",
    "                                    [noise_dim/2], \n",
    "                                    initializer = tf.truncated_normal_initializer(stddev = 0.01))\n",
    "        gen_network1 = tf.nn.conv2d(input = gen_network0, \n",
    "                                    filter = gen_weight1, \n",
    "                                    strides = [1, 2, 2, 1], \n",
    "                                    padding = \"SAME\")\n",
    "        gen_network1 = gen_network1 + gen_bias1\n",
    "        gen_network1 = tf.contrib.layers.batch_norm(gen_network1, \n",
    "                                                    epsilon = 1e-5, \n",
    "                                                   scope = \"bn1\")\n",
    "        gen_network1 = tf.nn.relu(gen_network1)\n",
    "        gen_network1 = tf.image.resize_images(gen_network1, [56, 56])\n",
    "\n",
    "        ## deconvolutional neural network 2\n",
    "        gen_weight2 = tf.get_variable(\"gen_weight2\", \n",
    "                                      [3, 3, noise_dim/2, noise_dim/4], \n",
    "                                      initializer = tf.truncated_normal_initializer(stddev=0.01), \n",
    "                                      dtype = tf.float32)\n",
    "        gen_bias2 = tf.get_variable(\"gen_bias2\", \n",
    "                                    [noise_dim/4], \n",
    "                                    initializer = tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "        gen_network2 = tf.nn.conv2d(input = gen_network1, \n",
    "                                    filter = gen_weight2, \n",
    "                                    padding = \"SAME\", \n",
    "                                    strides = [1, 2, 2, 1]) + gen_bias2\n",
    "        gen_network2 = tf.contrib.layers.batch_norm(gen_network2, \n",
    "                                                    epsilon = 1e-5, \n",
    "                                                    scope = \"bn2\")\n",
    "        gen_network2 = tf.nn.relu(gen_network2)\n",
    "        gen_network2 = tf.image.resize_images(gen_network2, [56, 56])\n",
    "\n",
    "        ## Producing one output dimensional channel\n",
    "        gen_weight3 = tf.get_variable(\"gen_weight3\", \n",
    "                                      [1, 1, noise_dim/4, 1], \n",
    "                                      initializer = tf.truncated_normal_initializer(stddev = 0.01), \n",
    "                                      dtype = tf.float32)\n",
    "        gen_bias3 = tf.get_variable(\"gen_bias3\", \n",
    "                                    [1], \n",
    "                                    initializer = tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "        gen_network3 = tf.nn.conv2d(input = gen_network2, \n",
    "                                    filter = gen_weight3, \n",
    "                                    strides = [1, 2, 2, 1], \n",
    "                                    padding = \"SAME\") + gen_bias3\n",
    "        gen_network3 = tf.sigmoid(gen_network3)\n",
    "\n",
    "        return gen_network3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf_sess = tf.Session()\n",
    "\n",
    "batches = 50\n",
    "noise_dim = 100\n",
    "\n",
    "X_placeholder = tf.placeholder(\"float\", shape = [None, 28, 28, 1], name = \"X_placeholder\")\n",
    "\n",
    "gen_func = generator(batches, noise_dim)\n",
    "disc_func = discriminator(X_placeholder)\n",
    "disc_gen_func = discriminator(gen_func)\n",
    "\n",
    "cross_entropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits = disc_gen_func, \n",
    "                                                             labels = tf.ones_like(disc_gen_func))\n",
    "\n",
    "gen_total_loss = tf.reduce_mean(cross_entropy_loss)\n",
    "\n",
    "disc_X_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_func, \n",
    "                                                                     labels=tf.fill([batches, 1], 0.9)))\n",
    "disc_gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen_func, \n",
    "                                                                       labels=tf.zeros_like(disc_gen_func)))\n",
    "disc_total_loss = disc_X_loss + disc_gen_loss\n",
    "\n",
    "train_variables = tf.trainable_variables()\n",
    "\n",
    "disc_variables = [var for var in train_variables if \"disc_\" in var.name]\n",
    "gen_variables = [var for var in train_variables if \"gen_\" in var.name]\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "    disc_train_X = tf.train.AdamOptimizer(0.001).minimize(disc_X_loss, var_list = disc_variables)\n",
    "    disc_train_fool_disc_gen = tf.train.AdamOptimizer(0.001).minimize(disc_gen_loss, var_list = disc_variables)\n",
    "    gen_train = tf.train.AdamOptimizer(0.001).minimize(gen_total_loss, var_list=gen_variables)\n",
    "    \n",
    "disc_count_real = tf.placeholder(tf.float32)\n",
    "disc_count_fake = tf.placeholder(tf.float32)\n",
    "gen_count = tf.placeholder(tf.float32)\n",
    "disc_after_train_fool = tf.reduce_mean(discriminator(generator(batches, noise_dim)))\n",
    "disc_after_train_real = tf.reduce_mean(discriminator(X_placeholder))\n",
    "images_tf = generator(batches, noise_dim)\n",
    "\n",
    "tf.summary.scalar(\"Generator_loss\", gen_total_loss)\n",
    "tf.summary.scalar(\"Discriminator_fake_loss\", disc_gen_loss)\n",
    "tf.summary.scalar(\"Discriminator_real_loss\", disc_X_loss)\n",
    "tf.summary.scalar(\"discriminator_count_real\", disc_count_real)\n",
    "tf.summary.scalar(\"discriminator_count_fake\", disc_count_fake)\n",
    "tf.summary.scalar(\"generator_count\", gen_count)\n",
    "tf.summary.scalar(\"discriminator_values_of_generated\", disc_after_train_fool)\n",
    "tf.summary.scalar(\"discriminator_values_of_real\", disc_after_train_real)\n",
    "tf.summary.image(\"generated_fake_images\", images_tf)\n",
    "\n",
    "merge = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/gan/\"\n",
    "writer = tf.summary.FileWriter(logdir, graph = tf_sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard/gan/\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.summary.writer.writer.FileWriter'>\n",
      "Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(logdir)\n",
    "print(type(merge))\n",
    "print(type(writer))\n",
    "print(disc_gen_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
